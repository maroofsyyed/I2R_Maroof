# -*- coding: utf-8 -*-
"""a2c_hebb_lin_lin_antbullet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zlSPYLpbmHhPzLXCWuHUBR0AYbjw_GlQ
"""



import gym
import pybullet_envs
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.distributions import Normal
from torch.utils.tensorboard import SummaryWriter
from gym.wrappers import RecordVideo, RecordEpisodeStatistics

# Set the CUDA device
device = torch.device("cuda:6" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Define the ModulatedPlasticDense layer
class ModulatedPlasticDense(nn.Module):
    def __init__(self, in_features, out_features, clip=2.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.clip = clip

        self.weight = nn.Parameter(torch.Tensor(in_features + 1, out_features))
        self.alpha = nn.Parameter(torch.Tensor(in_features + 1, out_features))

        self.ln = nn.LayerNorm(out_features)

        self.modulator = nn.Linear(out_features, 1)
        self.modfanout = nn.Linear(1, out_features)

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.normal_(self.weight, std=0.001)
        nn.init.normal_(self.alpha, std=0.001)

    def forward(self, x, hebb):
        x = F.pad(x, (0, 1), "constant", 1.0)  # bias
        weight = self.weight + self.alpha * hebb
        y = torch.tanh(self.ln((x.unsqueeze(1) @ weight).squeeze(1)))

        m = torch.tanh(self.modulator(y))
        eta = self.modfanout(m.unsqueeze(2))
        delta = eta * (x.unsqueeze(2) @ y.unsqueeze(1))
        hebb = torch.clamp(hebb + delta, min=-self.clip, max=self.clip)

        return y, m, hebb

# Define the Actor-Critic network with MPD layer
class ActorCritic(nn.Module):
    def __init__(self, num_inputs, num_actions, hidden_size=256):
        super(ActorCritic, self).__init__()

        self.actor_mpd = ModulatedPlasticDense(num_inputs, hidden_size)
        self.actor = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Linear(256, num_actions),
        )

        self.critic_mpd = ModulatedPlasticDense(num_inputs, hidden_size)
        self.critic = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

        self.log_std = nn.Parameter(torch.zeros(1, num_actions))

    def forward(self, x, actor_hebb, critic_hebb):
        actor_out, actor_m, actor_hebb = self.actor_mpd(x, actor_hebb)
        mu = self.actor(actor_out)
        std = self.log_std.exp().expand_as(mu)
        dist = Normal(mu, std)

        critic_out, critic_m, critic_hebb = self.critic_mpd(x, critic_hebb)
        value = self.critic(critic_out)

        return dist, value, actor_hebb, critic_hebb

# The rest of the code remains the same
def make_env(env_name, seed, capture_video, run_name):
    def thunk():
        env = gym.make(env_name)
        env = RecordEpisodeStatistics(env)
        if capture_video:
            video_trigger = lambda step: step % 9900 == 0
            env = RecordVideo(env, video_folder=f"videos/{run_name}", step_trigger=video_trigger, video_length=500)
        env.seed(seed)
        env.action_space.seed(seed)
        env.observation_space.seed(seed)
        return env
    return thunk()


def a2c(env_name, num_episodes=2000, gamma=0.99, lr=1e-4, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5, capture_video=True, run_name="a2c_antbullet_hebb"):
    env = make_env(env_name, seed=42, capture_video=capture_video, run_name=run_name)
    num_inputs = env.observation_space.shape[0]
    num_actions = env.action_space.shape[0]

    model = ActorCritic(num_inputs, num_actions).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    writer = SummaryWriter(comment=f"-{env_name}-A2C")

    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.9)

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0
        episode_length = 0

        actor_hebb = torch.zeros(num_inputs + 1, 256).to(device)
        critic_hebb = torch.zeros(num_inputs + 1, 256).to(device)

        while not done:
            state = torch.FloatTensor(state).unsqueeze(0).to(device)

            with torch.no_grad():
                dist, value, actor_hebb, critic_hebb = model(state, actor_hebb, critic_hebb)
                action = dist.sample()

            next_state, reward, done, _ = env.step(action.cpu().numpy()[0])

            total_reward += reward
            episode_length += 1

            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)

            with torch.no_grad():
                _, next_value, _, _ = model(next_state, actor_hebb, critic_hebb)

            advantage = reward + (gamma * next_value * (1 - done)) - value

            # Recompute the current state's distribution and value for backpropagation
            dist, value, actor_hebb, critic_hebb = model(state, actor_hebb, critic_hebb)

            actor_loss = -dist.log_prob(action).mean() * advantage.detach()
            critic_loss = value_loss_coef * advantage.pow(2).mean()
            entropy_loss = -entropy_coef * dist.entropy().mean()

            loss = actor_loss + critic_loss + entropy_loss

            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()

            state = next_state.squeeze(0).cpu().numpy()

        scheduler.step()

        writer.add_scalar('Total Reward', total_reward, episode)
        writer.add_scalar('Episode Length', episode_length, episode)
        writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], episode)

        print(f"Episode {episode + 1}, Total Reward: {total_reward}, Length: {episode_length}")

        if total_reward > 2000:
            print(f"Solved in {episode + 1} episodes!")
            break

    writer.close()
    env.close()


# Run the algorithm
env_name = "AntBulletEnv-v0"
a2c(env_name)