# -*- coding: utf-8 -*-
"""a2c_lin_lin_lin_antbullet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zlSPYLpbmHhPzLXCWuHUBR0AYbjw_GlQ
"""



import gym
import pybullet_envs
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.distributions import Normal
from torch.utils.tensorboard import SummaryWriter

# Set the CUDA device
device = torch.device("cuda:7" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Define the Actor-Critic network
class ActorCritic(nn.Module):
    def __init__(self, num_inputs, num_actions, hidden_size=256):
        super(ActorCritic, self).__init__()

        self.actor = nn.Sequential(
            nn.Linear(num_inputs, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_actions),
        )

        self.critic = nn.Sequential(
            nn.Linear(num_inputs, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

        self.log_std = nn.Parameter(torch.zeros(1, num_actions))

    def forward(self, x):
        value = self.critic(x)
        mu = self.actor(x)
        std = self.log_std.exp().expand_as(mu)
        dist = Normal(mu, std)
        return dist, value

# A2C algorithm with improvements
def a2c(env_name, num_episodes=1000, gamma=0.98, lr=1e-4, entropy_coef=0.01, value_loss_coef=0.5, max_grad_norm=0.5):
    env = gym.make(env_name)
    num_inputs = env.observation_space.shape[0]
    num_actions = env.action_space.shape[0]

    model = ActorCritic(num_inputs, num_actions).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    writer = SummaryWriter(comment=f"-{env_name}-A2C")

    # Implement a simple learning rate scheduler
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.9)

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0
        episode_length = 0

        while not done:
            state = torch.FloatTensor(state).unsqueeze(0).to(device)
            dist, value = model(state)
            action = dist.sample()
            next_state, reward, done, _ = env.step(action.cpu().numpy()[0])

            total_reward += reward
            episode_length += 1

            # Compute advantage
            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)
            _, next_value = model(next_state)
            advantage = reward + (gamma * next_value * (1 - done)) - value

            # Compute losses
            actor_loss = -dist.log_prob(action).mean() * advantage.detach()
            critic_loss = value_loss_coef * advantage.pow(2).mean()
            entropy_loss = -entropy_coef * dist.entropy().mean()

            loss = actor_loss + critic_loss + entropy_loss

            # Optimize the model
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()

            state = next_state.squeeze(0).cpu().numpy()

        # Update learning rate
        scheduler.step()

        # Log metrics
        writer.add_scalar('Total Reward', total_reward, episode)
        writer.add_scalar('Episode Length', episode_length, episode)
        writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], episode)

        print(f"Episode {episode + 1}, Total Reward: {total_reward}, Length: {episode_length}")

        # Early stopping
        if total_reward > 2000:  # Adjust this threshold based on the environment
            print(f"Solved in {episode + 1} episodes!")
            break

    writer.close()
    env.close()

# Run the algorithm
env_name = "AntBulletEnv-v0"
a2c(env_name)

