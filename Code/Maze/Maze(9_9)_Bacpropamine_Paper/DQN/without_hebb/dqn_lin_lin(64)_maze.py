# -*- coding: utf-8 -*-
"""dqn_lin_lin(64)_maze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zlSPYLpbmHhPzLXCWuHUBR0AYbjw_GlQ
"""

import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

# ANSI color codes
BLUE = "\033[44m"
RED = "\033[41m"
GREEN = "\033[42m"
CYAN = "\033[46m"
WHITE = "\033[37m"
RESET = "\033[0m"

# Check if CUDA is available and set the device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class MazeEnvironment:
    def __init__(self, size=11):
        self.size = size
        self.maze = self.create_maze()
        self.agent_position = None
        self.reward_position = None
        self.steps = 0
        self.max_steps = 200
        self.reset()

    def create_maze(self):
        maze = np.ones((self.size, self.size))
        maze[1:self.size-1, 1:self.size-1] = 0
        for row in range(1, self.size - 1):
            for col in range(1, self.size - 1):
                if row % 2 == 0 and col % 2 == 0:
                    maze[row, col] = 1
        maze[self.size//2, self.size//2] = 0
        return maze

    def reset(self):
        self.steps = 0
        self.agent_position = self.get_random_empty_position()
        self.reward_position = self.get_random_empty_position()
        return self.get_observation()

    def get_random_empty_position(self):
        empty_positions = np.argwhere(self.maze == 0)
        return tuple(random.choice(empty_positions))

    def get_observation(self):
        i, j = self.agent_position
        neighborhood = self.maze[max(0, i-1):i+2, max(0, j-1):j+2]
        if neighborhood.shape != (3, 3):
            padding = [(0, 3 - neighborhood.shape[0]), (0, 3 - neighborhood.shape[1])]
            neighborhood = np.pad(neighborhood, padding, mode='constant', constant_values=1)
        return neighborhood.flatten()

    def step(self, action):
        self.steps += 1
        i, j = self.agent_position

        if action == 0:  # up
            new_position = (max(0, i - 1), j)
        elif action == 1:  # right
            new_position = (i, min(self.size - 1, j + 1))
        elif action == 2:  # down
            new_position = (min(self.size - 1, i + 1), j)
        elif action == 3:  # left
            new_position = (i, max(0, j - 1))

        if self.maze[new_position] == 0:
            self.agent_position = new_position

        reward = 0
        done = False

        if self.agent_position == self.reward_position:
            reward = 10.0
            self.agent_position = self.get_random_empty_position()
        elif self.maze[new_position] == 1:
            reward = -0.1

        if self.steps >= self.max_steps:
            done = True

        return self.get_observation(), reward, done

    def render(self):
        render_maze = self.maze.copy()
        i, j = self.agent_position
        ri, rj = self.reward_position
        render_maze[i, j] = 2  # Agent
        render_maze[ri, rj] = 3  # Reward

        print(WHITE + "+" + "---+" * self.size + RESET)
        for row in render_maze:
            print(WHITE + "|", end="")
            for cell in row:
                if cell == 1:
                    print(BLUE + "   " + RESET, end=WHITE + "|")
                elif cell == 2:
                    print(RED + " A " + RESET, end=WHITE + "|")
                elif cell == 3:
                    print(GREEN + " R " + RESET, end=WHITE + "|")
                else:
                    print(CYAN + "   " + RESET, end=WHITE + "|")
            print("\n" + WHITE + "+" + "---+" * self.size + RESET)


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)

    def size(self):
        return len(self.buffer)


class DQN(nn.Module):
    def __init__(self, input_size, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(input_size, 128)
        self.layer2 = nn.Linear(128, n_actions)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.layer2(x)
        return x


class DQNAgent:
    def __init__(self, input_size, n_actions, lr=0.0001, gamma=0.98, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.device = device
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        self.q_network = DQN(input_size, n_actions).to(self.device)
        self.target_q_network = DQN(input_size, n_actions).to(self.device)
        self.target_q_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

        self.replay_buffer = ReplayBuffer(100000)
        self.batch_size = 64
        self.target_update_freq = 1000

    def select_action(self, state):
        if random.random() < self.epsilon:
            return random.randint(0, 3)
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state)
        return q_values.argmax().item()

    def update(self):
        if self.replay_buffer.size() < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        next_q_values = self.target_q_network(next_states)
        max_next_q_values = next_q_values.max(1)[0]
        expected_q_values = rewards + (self.gamma * max_next_q_values * (1 - dones))

        loss = nn.MSELoss()(current_q_values, expected_q_values.detach())

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay

    def update_target_network(self):
        self.target_q_network.load_state_dict(self.q_network.state_dict())


# Main training loop
env = MazeEnvironment()
agent = DQNAgent(input_size=9, n_actions=4)

num_episodes = 100000

for episode in range(num_episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)
        agent.replay_buffer.add(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward

        agent.update()

    agent.update_target_network()

    if (episode + 1) % 1 == 0:
        print(f"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}")