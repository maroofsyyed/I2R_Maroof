# -*- coding: utf-8 -*-
"""dqn_lin_lin_lin_maze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zlSPYLpbmHhPzLXCWuHUBR0AYbjw_GlQ
"""



import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

# ANSI color codes
BLUE = "\033[44m"
RED = "\033[41m"
GREEN = "\033[42m"
CYAN = "\033[46m"
WHITE = "\033[37m"
RESET = "\033[0m"

# Check if CUDA is available and set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class MazeEnvironment:
    def __init__(self, size=11):
        self.size = size
        self.maze = self.create_maze()
        self.agent_position = None
        self.reward_position = None
        self.steps = 0
        self.max_steps = 250
        self.reset()

    def create_maze(self):
        maze = np.ones((self.size, self.size))
        maze[1:self.size-1, 1:self.size-1] = 0
        for row in range(1, self.size - 1):
            for col in range(1, self.size - 1):
                if row % 2 == 0 and col % 2 == 0:
                    maze[row, col] = 1
        maze[self.size//2, self.size//2] = 0
        return maze

    def reset(self):
        self.steps = 0
        self.agent_position = self.get_random_empty_position()
        self.reward_position = self.get_random_empty_position()
        return self.get_observation()

    def get_random_empty_position(self):
        empty_positions = np.argwhere(self.maze == 0)
        return tuple(random.choice(empty_positions))

    def get_observation(self):
        i, j = self.agent_position
        neighborhood = self.maze[max(0, i-1):i+2, max(0, j-1):j+2]
        if neighborhood.shape != (3, 3):
            padding = [(0, 3 - neighborhood.shape[0]), (0, 3 - neighborhood.shape[1])]
            neighborhood = np.pad(neighborhood, padding, mode='constant', constant_values=1)
        return neighborhood.flatten()

    def step(self, action):
        self.steps += 1
        i, j = self.agent_position

        if action == 0:  # up
            new_position = (max(0, i - 1), j)
        elif action == 1:  # right
            new_position = (i, min(self.size - 1, j + 1))
        elif action == 2:  # down
            new_position = (min(self.size - 1, i + 1), j)
        elif action == 3:  # left
            new_position = (i, max(0, j - 1))

        if self.maze[new_position] == 0:
            self.agent_position = new_position

        reward = 0
        done = False

        if self.agent_position == self.reward_position:
            reward = 10.0
            self.agent_position = self.get_random_empty_position()
        elif self.maze[new_position] == 1:
            reward = -0.1

        if self.steps >= self.max_steps:
            done = True

        return self.get_observation(), reward, done

    def render(self):
        render_maze = self.maze.copy()
        i, j = self.agent_position
        ri, rj = self.reward_position
        render_maze[i, j] = 2  # Agent
        render_maze[ri, rj] = 3  # Reward

        print(WHITE + "+" + "---+" * self.size + RESET)
        for row in render_maze:
            print(WHITE + "|", end="")
            for cell in row:
                if cell == 1:
                    print(BLUE + "   " + RESET, end=WHITE + "|")
                elif cell == 2:
                    print(RED + " A " + RESET, end=WHITE + "|")
                elif cell == 3:
                    print(GREEN + " R " + RESET, end=WHITE + "|")
                else:
                    print(CYAN + "   " + RESET, end=WHITE + "|")
            print("\n" + WHITE + "+" + "---+" * self.size + RESET)


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)

    def size(self):
        return len(self.buffer)

class DQN(nn.Module):
    def __init__(self, input_size, n_actions):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, n_actions)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class DQNAgent:
    def __init__(self, input_size, n_actions, lr=0.0001, gamma=0.98, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.device = device
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        self.q_network = DQN(input_size, n_actions).to(self.device)
        self.target_q_network = DQN(input_size, n_actions).to(self.device)
        self.target_q_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

        self.replay_buffer = ReplayBuffer(1000000)
        self.batch_size = 64
        self.update_target_frequency = 1000
        self.update_count = 0
        self.n_actions = n_actions

    def get_action(self, state):
        if random.random() < self.epsilon:
            return random.randint(0, self.n_actions - 1)
        else:
            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state)
            return torch.argmax(q_values).item()

    def update(self):
        if self.replay_buffer.size() < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)

        current_q_values = self.q_network(states).gather(1, actions)
        next_q_values = self.target_q_network(next_states).max(1)[0].unsqueeze(1)
        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values

        loss = nn.MSELoss()(current_q_values, target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.update_count += 1
        if self.update_count % self.update_target_frequency == 0:
            self.target_q_network.load_state_dict(self.q_network.state_dict())

        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

def train(env, agent, n_episodes=100000, max_steps=250):
    rewards_history = []

    for episode in range(n_episodes):
        state = env.reset()
        total_reward = 0
        done = False
        step = 0

        while not done and step < max_steps:
            action = agent.get_action(state)
            next_state, reward, done = env.step(action)

            agent.replay_buffer.add(state, action, reward, next_state, done)

            state = next_state
            total_reward += reward
            step += 1

            agent.update()

        rewards_history.append(total_reward)

        if episode % 100 == 0:
            avg_reward = np.mean(rewards_history[-100:])
            print(f"Episode {episode}, Average Reward: {avg_reward:.2f}")

    return rewards_history

# Create environment and agent
env = MazeEnvironment(size=11)
input_size = 9  # 3x3 neighborhood
n_actions = 4  # up, right, down, left
agent = DQNAgent(input_size, n_actions)

# Train the agent
rewards_history = train(env, agent)

# Test the trained agent
state = env.reset()
done = False
total_reward = 0
step = 0

while not done and step < env.max_steps:
    env.render()
    action = agent.get_action(state)
    state, reward, done = env.step(action)
    total_reward += reward
    step += 1

print(f"Test episode finished. Total reward: {total_reward}")



