# -*- coding: utf-8 -*-
"""dqn_hebb_lin(64)_maze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zlSPYLpbmHhPzLXCWuHUBR0AYbjw_GlQ
"""

import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

# ANSI color codes
BLUE = "\033[44m"
RED = "\033[41m"
GREEN = "\033[42m"
CYAN = "\033[46m"
WHITE = "\033[37m"
RESET = "\033[0m"

# Check if CUDA is available and set the device
device = torch.device("cuda:7" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class MazeEnvironment:
    def __init__(self, size=11):
        self.size = size
        self.maze = self.create_maze()
        self.agent_position = None
        self.reward_position = None
        self.steps = 0
        self.max_steps = 200
        self.reset()

    def create_maze(self):
        maze = np.ones((self.size, self.size))
        maze[1:self.size-1, 1:self.size-1] = 0
        for row in range(1, self.size - 1):
            for col in range(1, self.size - 1):
                if row % 2 == 0 and col % 2 == 0:
                    maze[row, col] = 1
        maze[self.size//2, self.size//2] = 0
        return maze

    def reset(self):
        self.steps = 0
        self.agent_position = self.get_random_empty_position()
        self.reward_position = self.get_random_empty_position()
        return self.get_observation()

    def get_random_empty_position(self):
        empty_positions = np.argwhere(self.maze == 0)
        return tuple(random.choice(empty_positions))

    def get_observation(self):
        i, j = self.agent_position
        neighborhood = self.maze[max(0, i-1):i+2, max(0, j-1):j+2]
        if neighborhood.shape != (3, 3):
            padding = [(0, 3 - neighborhood.shape[0]), (0, 3 - neighborhood.shape[1])]
            neighborhood = np.pad(neighborhood, padding, mode='constant', constant_values=1)
        return neighborhood.flatten()

    def step(self, action):
        self.steps += 1
        i, j = self.agent_position

        if action == 0:  # up
            new_position = (max(0, i - 1), j)
        elif action == 1:  # right
            new_position = (i, min(self.size - 1, j + 1))
        elif action == 2:  # down
            new_position = (min(self.size - 1, i + 1), j)
        elif action == 3:  # left
            new_position = (i, max(0, j - 1))

        if self.maze[new_position] == 0:
            self.agent_position = new_position

        reward = 0
        done = False

        if self.agent_position == self.reward_position:
            reward = 10.0
            self.agent_position = self.get_random_empty_position()
        elif self.maze[new_position] == 1:
            reward = -0.1

        if self.steps >= self.max_steps:
            done = True

        return self.get_observation(), reward, done

    def render(self):
        render_maze = self.maze.copy()
        i, j = self.agent_position
        ri, rj = self.reward_position
        render_maze[i, j] = 2  # Agent
        render_maze[ri, rj] = 3  # Reward

        print(WHITE + "+" + "---+" * self.size + RESET)
        for row in render_maze:
            print(WHITE + "|", end="")
            for cell in row:
                if cell == 1:
                    print(BLUE + "   " + RESET, end=WHITE + "|")
                elif cell == 2:
                    print(RED + " A " + RESET, end=WHITE + "|")
                elif cell == 3:
                    print(GREEN + " R " + RESET, end=WHITE + "|")
                else:
                    print(CYAN + "   " + RESET, end=WHITE + "|")
            print("\n" + WHITE + "+" + "---+" * self.size + RESET)


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)

    def size(self):
        return len(self.buffer)

class ModulatedPlasticDense(nn.Module):
    def __init__(self, in_features, out_features, clip=2.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.clip = clip #used to limit the range of the Hebbian values

        self.weight = nn.Parameter(torch.Tensor(in_features + 1, out_features)) #tensor that represents the weight matrix of the fully connected layer. The extra dimension is for the bias term.
        self.alpha = nn.Parameter(torch.Tensor(in_features + 1, out_features)) #tensor represents the plasticity coefficients for each connection in the layer.

        self.ln = nn.LayerNorm(out_features) #module for normalizing the output of the layer.

        self.modulator = nn.Linear(out_features, 1) #A linear layer (nn.Linear) that takes the output of the current layer and produces a scalar modulation factor for each neuron.
        self.modfanout = nn.Linear(1, out_features)  # per-neuron #Another linear layer (nn.Linear) that takes the scalar modulation factors and produces per-neuron modulation factors.

        self.reset_parameters() #to initialize the weights and plasticity coefficients with small random values.

    #initializes the self.weight and self.alpha parameters with small random values drawn from a normal distribution with a standard deviation of 0.001.

    def reset_parameters(self):
        nn.init.normal_(self.weight, std=0.001)
        nn.init.normal_(self.alpha, std=0.001)

    def forward(self, x, hebb): # two inputs: x (the input tensor) and hebb (the Hebbian values tensor, which has the same shape as self.weight).
        x = torch.nn.functional.pad(x, (0, 1), "constant", 1.0)  # bias
        weight = self.weight + self.alpha * hebb
        y = torch.tanh(self.ln((x.unsqueeze(1) @ weight).squeeze(1))) #is the sum of self.weight and the product of self.alpha and hebb (element-wise multiplication) aka modulated plasticity part
        # neuromodulated plasticity update
        m = torch.tanh(self.modulator(y))
        eta = self.modfanout(m.unsqueeze(2))
        delta = eta * (x.unsqueeze(2) @ y.unsqueeze(1)) # The Hebbian update delta is computed as eta * (x.unsqueeze(2) @ y.unsqueeze(1)), which is the outer product of the input and output tensors, modulated by eta.
        hebb = torch.clamp(hebb + delta, min=-self.clip, max=self.clip) #The Hebbian values hebb are updated by adding delta and clipping the result between -self.clip and self.clip.
        hebb = hebb.squeeze(0)
        return y, m, hebb #returns the output y, the scalar modulation factors m, and the updated Hebbian values hebb.

class DQN(nn.Module):
    def __init__(self, input_size, n_actions):
        super(DQN, self).__init__()
        self.layer1 = ModulatedPlasticDense(input_size, 64)
        self.layer2 = nn.Linear(64, n_actions)

    def forward(self, x, hebb):
        if hebb is None:
            hebb = torch.zeros(x.size(0), self.layer1.in_features + 1, self.layer1.out_features).to(x.device)
        x, m, hebb = self.layer1(x, hebb)
        x = self.layer2(x)
        return x, m, hebb

class DQNAgent:
    def __init__(self, input_size, n_actions, lr=0.0001, gamma=0.98, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.device = device
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        self.q_network = DQN(input_size, n_actions).to(self.device)
        self.target_q_network = DQN(input_size, n_actions).to(self.device)
        self.target_q_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

        self.replay_buffer = ReplayBuffer(1000000)
        self.batch_size = 64
        self.target_update_freq = 1000

    def select_action(self, state):
        if random.random() < self.epsilon:
            return random.randint(0, 3)
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values, _, _ = self.q_network(state, None)
        return q_values.argmax().item()

    def update(self):
        if self.replay_buffer.size() < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        current_q_values, _, _ = self.q_network(states, None)
        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)

        next_q_values, _, _ = self.target_q_network(next_states, None)
        max_next_q_values = next_q_values.max(1)[0]
        expected_q_values = rewards + (self.gamma * max_next_q_values * (1 - dones))

        loss = nn.MSELoss()(current_q_values, expected_q_values.detach())

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay

    def update_target_network(self):
        self.target_q_network.load_state_dict(self.q_network.state_dict())

# Main training loop
env = MazeEnvironment()
agent = DQNAgent(input_size=9, n_actions=4)

num_episodes = 100000

for episode in range(num_episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)
        agent.replay_buffer.add(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward

        agent.update()

    agent.update_target_network()

    if (episode + 1) % 1 == 0:
        print(f"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}")