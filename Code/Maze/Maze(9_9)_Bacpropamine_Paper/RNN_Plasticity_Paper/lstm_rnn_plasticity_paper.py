# -*- coding: utf-8 -*-
"""LSTM_rnn_plasticity paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hjiuypGWGybErVNne28alD_kG4gDrab8
"""



import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
import random
import time

# Constants
ETA = .02  # Not used
ADDINPUT = 4  # 1 input for the previous reward, 1 input for numstep, 1 for whether currently on reward square, 1 "Bias" input
NBACTIONS = 4  # U, D, L, R
RFSIZE = 3  # Receptive field size
TOTALNBINPUTS = RFSIZE * RFSIZE + ADDINPUT + NBACTIONS

# For GPU
ttype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor

class Network(nn.Module):
    def __init__(self, params):
        super(Network, self).__init__()
        self.rule = params['rule']
        self.type = params['type']
        self.softmax = torch.nn.functional.softmax
        if params['activ'] == 'tanh':
            self.activ = torch.tanh
        elif params['activ'] == 'selu':
            self.activ = torch.selu
        else:
            raise ValueError('Must choose an activ function')
        if params['type'] == 'lstm':
            self.lstm = torch.nn.LSTM(TOTALNBINPUTS, params['hiddensize']).cuda() if torch.cuda.is_available() else torch.nn.LSTM(TOTALNBINPUTS, params['hiddensize'])
        else:
            raise ValueError("Which network type?")
        self.h2o = torch.nn.Linear(params['hiddensize'], NBACTIONS).cuda() if torch.cuda.is_available() else torch.nn.Linear(params['hiddensize'], NBACTIONS)
        self.h2v = torch.nn.Linear(params['hiddensize'], 1).cuda() if torch.cuda.is_available() else torch.nn.Linear(params['hiddensize'], 1)
        self.params = params

    def forward(self, input, hidden, hebb):
        if self.type == 'lstm':
            hactiv, hidden = self.lstm(input.view(1, 1, -1), hidden)  # hactiv is just the h. hidden is the h and the cell state, in a tuple
            hactiv = hactiv.view(1, -1)

        activout = self.softmax(self.h2o(hactiv), dim=1)   # Action selection
        valueout = self.h2v(hactiv)                        # Value prediction (for A3C)

        return activout, valueout, hidden, hebb

    def initialZeroHebb(self):
        return Variable(torch.zeros(self.params['hiddensize'], self.params['hiddensize']), requires_grad=False).cuda() if torch.cuda.is_available() else Variable(torch.zeros(self.params['hiddensize'], self.params['hiddensize']), requires_grad=False)

    def initialZeroState(self):
        if self.params['type'] == 'lstm':
            return (Variable(torch.zeros(1, 1, self.params['hiddensize']), requires_grad=False).cuda(),
                    Variable(torch.zeros(1, 1, self.params['hiddensize']), requires_grad=False).cuda()) if torch.cuda.is_available() else (Variable(torch.zeros(1, 1, self.params['hiddensize']), requires_grad=False),
                    Variable(torch.zeros(1, 1, self.params['hiddensize']), requires_grad=False))
        else:
            raise ValueError("Which type?")


def train(params):
    print("Starting training...")
    print("Passed params: ", params)

    # Initialize random seeds
    np.random.seed(params['rngseed'])
    random.seed(params['rngseed'])
    torch.manual_seed(params['rngseed'])

    print("Initializing network")
    net = Network(params)
    print("Shape of all optimized parameters:", [x.size() for x in net.parameters()])
    allsizes = [torch.numel(x.data.cpu()) for x in net.parameters()]
    print("Size (numel) of all optimized elements:", allsizes)
    print("Total size (numel) of all optimized elements:", sum(allsizes))

    print("Initializing optimizer")
    optimizer = torch.optim.Adam(net.parameters(), lr=params['lr'], eps=1e-4)

    LABSIZE = params['labsize']
    lab = np.ones((LABSIZE, LABSIZE))
    CTR = LABSIZE // 2

    # Grid maze
    lab[1:LABSIZE-1, 1:LABSIZE-1].fill(0)
    for row in range(1, LABSIZE - 1):
        for col in range(1, LABSIZE - 1):
            if row % 2 == 0 and col % 2 == 0:
                lab[row, col] = 1
    lab[CTR, CTR] = 0  # Not really necessary, but nicer to not start on a wall, and perhaps helps localization by introducing a detectable irregularity in the center

    all_losses = []
    all_losses_objective = []
    all_losses_v = []
    all_rewards = []
    lossbetweensaves = 0
    nowtime = time.time()

    print("Starting episodes...")

    for numiter in range(params['nbiter']):

        PRINTTRACE = 0
        if (numiter + 1) % params['print_every'] == 0:
            PRINTTRACE = 1

        rposr, rposc = 0, 0
        if params['rp'] == 0:
            while lab[rposr, rposc] == 1:
                rposr = np.random.randint(1, LABSIZE - 1)
                rposc = np.random.randint(1, LABSIZE - 1)
        elif params['rp'] == 1:
            while lab[rposr, rposc] == 1 or (rposr != 1 and rposr != LABSIZE - 2 and rposc != 1 and rposc != LABSIZE - 2):
                rposr = np.random.randint(1, LABSIZE - 1)
                rposc = np.random.randint(1, LABSIZE - 1)

        posc, posr = CTR, CTR

        optimizer.zero_grad()
        loss, lossv = 0, 0
        hidden = net.initialZeroState()
        hebb = net.initialZeroHebb()

        reward = 0.0
        rewards = []
        vs = []
        logprobs = []
        sumreward = 0.0
        dist = 0

        for numstep in range(params['eplen']):
            inputsN = np.zeros((1, TOTALNBINPUTS), dtype='float32')
            inputsN[0, 0:RFSIZE * RFSIZE] = lab[posr - RFSIZE//2:posr + RFSIZE//2 + 1, posc - RFSIZE //2:posc + RFSIZE//2 + 1].flatten()

            inputs = torch.from_numpy(inputsN).cuda() if torch.cuda.is_available() else torch.from_numpy(inputsN)
            inputs[0][-1] = 1  # Bias neuron
            inputs[0][-2] = numstep
            inputs[0][-3] = reward

            y, v, hidden, hebb = net(Variable(inputs, requires_grad=False), hidden, hebb)  # y should output probabilities; v is the value prediction

            distrib = torch.distributions.Categorical(y)
            actionchosen = distrib.sample()  # sample() returns a Pytorch tensor of size 1; this is needed for the backprop below
            numactionchosen = actionchosen.item()  # Turn to scalar

            # Target position, based on the selected action
            tgtposc, tgtposr = posc, posr
            if numactionchosen == 0:  # Up
                tgtposr -= 1
            elif numactionchosen == 1:  # Down
                tgtposr += 1
            elif numactionchosen == 2:  # Left
                tgtposc -= 1
            elif numactionchosen == 3:  # Right
                tgtposc += 1
            else:
                raise ValueError("Wrong Action")

            reward = 0.0
            if lab[tgtposr][tgtposc] == 1:
                reward = -0.1
            else:
                dist += 1
                posc, posr = tgtposc, tgtposr

            if rposr == posr and rposc == posc:
                reward += 10
                if params['randstart'] == 1:
                    posr, posc = np.random.randint(1, LABSIZE - 1), np.random.randint(1, LABSIZE - 1)
                    while lab[posr, posc] == 1:
                        posr, posc = np.random.randint(1, LABSIZE - 1), np.random.randint(1, LABSIZE - 1)

            sumreward += reward

            rewards.append(reward)
            vs.append(v)
            logprobs.append(distrib.log_prob(actionchosen))

        R = torch.zeros(1, 1).cuda() if torch.cuda.is_available() else torch.zeros(1, 1)
        if not params['randstart']:
            R = v

        policy_loss = 0
        value_loss = 0
        gae = torch.zeros(1, 1).cuda() if torch.cuda.is_available() else torch.zeros(1, 1)
        for i in reversed(range(len(rewards))):
            R = R * params['gamma'] + rewards[i]
            advantage = R - vs[i]
            value_loss = value_loss + 0.5 * advantage.pow(2)
            delta_t = rewards[i] + params['gamma'] * vs[i + 1].data - vs[i].data if i < len(rewards) - 1 else rewards[i] + params['gamma'] * vs[i].data - vs[i].data
            gae = gae * params['gamma'] * params['tau'] + delta_t
            policy_loss = policy_loss - logprobs[i] * Variable(gae, requires_grad=False)

        optimizer.zero_grad()
        loss = policy_loss + params['value_coeff'] * value_loss
        loss.backward()
        optimizer.step()

        lossbetweensaves += loss.item()
        all_rewards.append(sumreward)

        if PRINTTRACE:
            print(f"Iteration: {numiter}, Reward: {sumreward}")

        if (numiter + 1) % params['save_every'] == 0:
            all_losses.append(lossbetweensaves / params['save_every'])
            all_losses_objective.append(policy_loss.item())
            all_losses_v.append(value_loss.item())
            lossbetweensaves = 0

        if (numiter + 1) % 1 == 0:
            avg_reward = np.mean(all_rewards[-100:])
            print(f"Average reward over last 100 episodes: {avg_reward}")

    return all_losses, all_losses_objective, all_losses_v


params = {
    'rule': 'hebb',
    'type': 'lstm',
    'activ': 'tanh',
    'hiddensize': 200,
    'lr': 0.0001,
    'gamma': 0.9,
    'tau': 1.0,
    'value_coeff': 0.5,
    'nbiter': 100000,
    'eplen': 100,
    'labsize': 11,
    'rngseed': 0,
    'print_every': 1,
    'save_every': 100,
    'randstart': 1,
    'rp': 1
}

# Start training
losses, losses_objective, losses_v = train(params)